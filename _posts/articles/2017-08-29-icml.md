---
layout: page
title: "10 Cool Papers from ICML"
teaser: "Did you miss ICML? Here are some cool papers from this year's conference."
header: no
comments: true
categories: article
published: false
---

Here are some papers that our group found interesting from this year's conference.

# 1. [Failures of Gradient-Based Deep Learning](https://arxiv.org/abs/1703.07950)

*Why it's cool:* We're used to seeing demonstrations of the success of deep learning on tasks ranging from image captioning to transcription factor binding prediction. But are there certain tasks that deep learning (and more generally, gradient-based methods) are inherently unsuited for? Shalev-Shwartz et. al show that there are certain problems (such as predicting the parity of a random vectors based on observing its dot product with known vectors), in which the loss function has similar values for the gradient regardless of which hypothesis is chosen from the set of hypotheses. Theoretically and empirically, this means that gradient-based methods will not be able to learn the true hypothesis, even with lots of time!

# 2. [Parseval Networks: Improving Robustness to Adversarial Examples](https://arxiv.org/abs/1704.08847)

*Why it's cool*: Part of the reason deep learning is able to model so many complex patterns between the input and output spaces is because of the non-linear transformations that are learned by the network. Each nonlinear transformation can be thought of "warping" the input space so that in the end, different classes of objects are easily separable in the warped space. However, this makes neural networks vulnerable to adversarial examples, whereby input images (or documents, etc.) are perturbed very slightly in the _unwarped_ space, but result very different labels predicted by the network. In this paper from Facebook AI Research, researchers make neural networks more robust by limiting the Lipschitz constant (a mathematical property that quantifies warpedness) of the network. Surprisingly, they find that this produces not only more robust networks, but neural networks that perform better even on the original images!

 # 3. [Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture](http://sleep.csail.mit.edu/files/rfsleep-paper.pdf)

*Why it's cool*: This work is an awesome synthesis of machine learning theory and a cutting edge wireless/medical application. Previous researchers from Dina Katabi's lab at MIT have developed a wireless device that can use reflected (or "backsacttered") Wifi signals to measure things like the heartbeat of people in a room. Here, they try to extend that to predict sleep stages of someone based _only_ on reflected wireless signals. The challenge is to do this in a way which adapts to different environments and subjects. They do this by building a encoder-predictor-discriminator network. The _encoder_ turns raw wireless signals into features, which the _predictor_ must use to predict sleep stages, but which the _discriminator_ is unable to use to discriminate between different settings and individuals. This way, they ensure that they learn features that are only relevant to predicting sleep stages, instead of domain-specific features.

 # 4. [When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests, l2-consistency and Neuroscience Applications](http://proceedings.mlr.press/v70/zhou17c/zhou17c.pdf)

*Why it's cool*: 

 # 5. [The Shattered Gradients Problem: If resnets are the answer, then what is the question?](https://arxiv.org/pdf/1702.08591.pdf)

*Why it's cool*: 

 # 6. [Asymmetric Tri-training for Unsupervised Domain Adaptation](https://arxiv.org/pdf/1702.08400.pdf)

*Why it's cool*: 

  
