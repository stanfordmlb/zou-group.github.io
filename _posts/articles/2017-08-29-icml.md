---
layout: page
title: "10 Cool Papers from ICML"
teaser: "Did you miss ICML? Here are some cool papers from this year's conference."
header: no
comments: true
categories: article
published: false
---

Here are some papers that our group found interesting from this year's conference.

# 1. [Failures of Gradient-Based Deep Learning](https://arxiv.org/abs/1703.07950)

*Why it's cool:* We're used to seeing demonstrations of the success of deep learning on tasks ranging from image captioning to transcription factor binding prediction. But are there certain tasks that deep learning (and more generally, gradient-based methods) are inherently unsuited for? Shalev-Shwartz et. al show that there are certain problems (such as predicting the parity of a random vectors based on observing its dot product with known vectors), in which the loss function has similar values for the gradient regardless of which hypothesis is chosen from the set of hypotheses. Theoretically and empirically, this means that gradient-based methods will not be able to learn the true hypothesis, even with lots of time!

# 2. [Parseval Networks: Improving Robustness to Adversarial Examples](https://arxiv.org/abs/1704.08847)

*Why it's cool*: Part of the reason deep learning is able to model so many complex patterns between the input and output spaces is because of the non-linear transformations that are learned by the network. Each nonlinear transformation can be thought of "warping" the input space so that in the end, different classes of objects are easily separable in the warped space. However, this makes neural networks vulnerable to adversarial examples, whereby input images (or documents, etc.) are perturbed very slightly in the _unwarped_ space, but result very different labels predicted by the network. In this paper from Facebook AI Research, researchers make neural networks more robust by limiting the Lipschitz constant (a mathematical property that quantifies warpedness) of the network. Surprisingly, they find that this produces not only more robust networks, but neural networks that perform better even on the original images!

 # 3. [Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture](http://sleep.csail.mit.edu/files/rfsleep-paper.pdf)

*Why it's cool*: This work is an awesome synthesis of machine learning theory and a cutting edge wireless/medical application. Previous researchers from Dina Katabi's lab at MIT have developed a wireless device that can use reflected (or "backsacttered") Wifi signals to measure things like the heartbeat of people in a room. Here, they try to extend that to predict sleep stages of someone based _only_ on reflected wireless signals. The challenge is to do this in a way which adapts to different environments and subjects. They do this by building a encoder-predictor-discriminator network. The _encoder_ turns raw wireless signals into features, which the _predictor_ must use to predict sleep stages, but which the _discriminator_ is unable to use to discriminate between different settings and individuals. This way, they ensure that they learn features that are only relevant to predicting sleep stages, instead of domain-specific features.

 # 4. [When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests, l2-consistency and Neuroscience Applications](http://proceedings.mlr.press/v70/zhou17c/zhou17c.pdf)

*Why it's cool*: Many studies in biomedical and health sciences involve small sample sizes due to logistic or financial constraints, which makes analysis hard and inaccurate. One common solution is to pool data from diverse labs directly together. However, is such pooling guaranteed to help? In this paper, researchers propose a hypothesis test to answer this question. Theoretical study is carried on a simple setting of linear regression to data with no model mismatch. The "bias-variance" trade-off is found to be the key point. When the data size is not large enough, multi-site formulation can decrease the vairance signigicantly, while only increase the bias by a little. The hypothesis test is then validated by emperical experiments on Alzheimer's disease studies.

 # 5. [The Shattered Gradients Problem: If resnets are the answer, then what is the question?](https://arxiv.org/pdf/1702.08591.pdf)

*Why it's cool*: Vanishing and exploding gradients were viewed as critical problems in deep learning. It was later well solved by carefully-chosen initialization and batch normalization. However, in recent researches, people found that architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures, without good explanations. This work points out a previously unnoticed problem which may be the hidden reason - the shattered gradients of deep neural network. As the network's depth increases, gradients in standard feedforward networks increasingly resemble white noise. In contrast, resnets dramatically reduce this shatter tendency of gradients.

 # 6. [Asymmetric Tri-training for Unsupervised Domain Adaptation](https://arxiv.org/pdf/1702.08400.pdf)

*Why it's cool*: This work achieves impressive performance for domain adaptation task, while not explicitly decreases the feature distributions of source and target domains. More specifically, it utilizes an asymmetric tri-training networks where two networks are used to label target data, and the last one is trainined only on those pseudo-labeled samples. Comprehensive experiments are carried out and demonstrate better performance over existing methods such as MMD and DANN. One interesting result is that this model only silghtly reduced the divergence of domain compared to other methods.

  
